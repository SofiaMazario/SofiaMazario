#cargamos el diccionario en inglés
!python -m spacy download en_core_web_sm

import spacy
nlp = spacy.load('en_core_web_sm')

# Instalamos todas las librerías y paquetes que necesitaremos
import sys
if 'google.colab' in sys.modules:
    !pip install emoji --upgrade
    !pip install pandas-profiling==3.6.0
    !pip install plotly==4.*
    !python -m spacy download en_core_web_sm
    !pip install pyLDAvis
    !pip install gensim
    !pip install nltk
    !pip install chart_studio
    !pip install --upgrade autopep8
    !pip install vaderSentiment
    !pip install datatable

#Instalamos otros paquetes, entre ellos numpy. Deberemos reiniciar la ejecución después de ejecutarlo
!pip install unidecode 
!pip install --upgrade pip
!pip install -U numpy #librería de manejo de datos - REINICIAR RUNTIME 

# Subimos el archivo para el pre-procesado de datos
from google.colab import files
uploaded = files.upload()
# Después de subirlo, cargamos el csv 
import pandas as pd
df_analisis = pd.read_csv('tweets_limpios.csv')

# Mostrar la columna 'tweet'
tweets = df_analisis['tweet']
print(tweets)

nltk.download('stopwords')
stopwords = set(nltk.corpus.stopwords.words('english'))

import nltk
from nltk.corpus import stopwords

# Descarga la lista de stopwords si aún no lo has hecho
nltk.download('stopwords')

# Ahora accedemos a las stopwords en inglés
stop_words = stopwords.words('english')
print(stop_words)

#Añadimos una serie de stopwords relacionadas con nuestro dataset particular, o bien porque se repiten mucho o porque no se han limpiado correctamente
stop_words.extend(['smoke','url','number','well', 'affffffffff','yes', 'lot','come','griffith','silicon','valley','investors','tell','way','use','must','year','say','hey','wanna', 'may', 'lol', 'amp','get','go','issuer','one','day','today','really','new','peoplir','need','think','got','el','much','video','dick', 'want','youaeur','last','said','hot','back','gotta','start',,'juice','gonna','night','thing','still','know','car', 'friends',,'fucking','look','take' ])

#realizamos el proceso de lematización y añadimos de forma manual aquellas que no se hayan realizado
nlp = spacy.load('en_core_web_sm')
def lemma_words(text):
    lemmas = []
    doc= nlp(text)
    for token in doc:
        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):
            lemmas.append(token.lemma_)
    lemmas= [i for i in lemmas if len(i) > 1]
    lemmas = [word for line in lemmas for word in line.split()]
    lemmas=[word for word in lemmas if word not in stop_words]
    lemmas=' '.join(lemmas)
    lemmas=unidecode(lemmas, "utf-8")
    lemmas=re.sub(r"\bsmoking\b", "smoke", lemmas)
    lemmas=re.sub(r"\bsmoking\b", "smoke", lemmas)
    lemmas=re.sub(r"\bvaping\b", "vape", lemmas)
    lemmas=re.sub(r"\bweeeeeeed\b", "weed", lemmas)
    lemmas=re.sub(r"\bfucking\b", "fuck", lemmas)
    lemmas=re.sub(r"\byouaeur\b", "you are", lemmas)
    lemmas=re.sub(r"\bcig\b", "cigarette", lemmas)
    lemmas=re.sub(r"\bcigar\b", "cigarette", lemmas)
    lemmas=re.sub(r"\bdonaEur\b", "do", lemmas)
    lemmas=re.sub(r"\byouaEUR\b", "you", lemmas)
    lemmas=re.sub(r"\bisnaEUR\b", "is", lemmas)
    lemmas=re.sub(r"\bitaEur\b", "it", lemmas)
    lemmas=re.sub(r"\bcanaEur\b", "can", lemmas)
    lemmas=re.sub(r"\bdrinking\b", "drink", lemmas)
    lemmas=re.sub(r"\blmao\b", "laugh", lemmas)
    lemmas=re.sub(r"\fda\b", "drug", lemmas)
    lemmas=re.sub(r"\bisnaEUR\b", "is", lemmas)
    lemmas=re.sub(r"\bdy\b", "do", lemmas)
    lemmas=re.sub(r"\bisnaEUR\b", "is", lemmas)
    lemmas=re.sub(r"\bweeeeeee\b", "weed", lemmas)
    lemmas=re.sub(r"\bfattt\b", "fat", lemmas)
    lemmas=re.sub(r"\bfuckkkk\b", "fuck", lemmas)
    lemmas=re.sub(r"\binvestors\b", "investor", lemmas)

    lemmas = lemmas.split()
    lemmas=[word for word in lemmas if word not in stop_words]
    lemmas=' '.join(lemmas)
    return lemmas

df_analisis['lemmas'] = df_analisis['tweet'].apply(lemma_words)

# Eliminamos filas con valores faltantes y filas duplicadas
df_analisis.dropna(inplace=True)
df_analisis = df_analisis.drop_duplicates(subset='lemmas', keep="first")

#Hacemos que nos guarde el resultado en un csv que podemos descargarnos
df_analisis['lemmas'].to_csv('Lemmas_tabaco.csv', index=False)

# Guardamos el resultado en un archivo Excel que puedes descargar
df_analisis.to_csv('Tabaco.csv', index=False)

##WORDCLOUD
# Definimos la función para poder plotear el wordcloud
def plot_cloud(wordcloud):
    # Señalamos el tamaño de la figura
    plt.figure(figsize=(10, 7))
    # Pedimos que nos la muestre
    plt.imshow(wordcloud)
    # Y también señalamos que no queremos que nos muestre detalle de los ejes
    plt.axis("off");

# Generamos el wordcloud
wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(' '.join(df_analisis['lemmas']))

#Lo ploteamos
plot_cloud(wordcloud)

#Cargamos nuestro csv en el DataFrame "df_analisis"
df_analisis = pd.read_csv('Tabaco.csv')
#Creamos una lista, dft, con los valores de la columna 'lemmas' del DataFrame df_example, excluyendo los valores "nan"
dft= df_analisis['lemmas']
dft = [x for x in dft if str(x) != 'nan']

#Esta es la función TF-IDF para los unigramas
tfIdfVectorizer=TfidfVectorizer(use_idf=True, ngram_range=(1,1))
tfIdf = tfIdfVectorizer.fit_transform(dft)
names=tfIdfVectorizer.get_feature_names_out()
freqs = tfIdf.sum(axis=0).A1
result= dict(zip(names, freqs))

# Obtenemos las 30 palabras más importantes aplicando TF-IF
from operator import itemgetter
i = 0
results_sorted=sorted(result.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 31:
      break
    print(key, value)

#Le pedimos que nos muestre los resultados y nos los plotee adecuadamente
df_results=pd.DataFrame.from_dict(results_sorted).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results[0],df_results[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Unigramas más relevantes del corpus')

#Esta es la función TF-IDF para los bigramas
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))

#Le pedimos que nos muestre los resultados y nos los plotee adecuadamente
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del corpus')

#Esta es la función TF-IDF para los trigramas
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))

#Le pedimos que nos muestre los resultados y nos los plotee adecuadamente
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del corpus')


##Modelado de tópicos
#Abrimos el csv que guardamos con anterioridad y pedimos que nos lo muestre 
df_analisis = pd.read_csv('Tabaco.csv')
print(df_analisis.head())

# Tokenizamos los datos
def tokenize(text):
    text = str(text)
    tokens = text.split() # Pedimos que nos lo convierta a minúscula y nos lo divida
    return tokens

# Aplicamos el tokenizer
df_analisis['tokens'] = df_analisis['lemmas'].apply(tokenize)
print(df_analisis.head())

#Importamos semilla para la reproducibilidad
seed(23)
#Creamos un diccionario de palabras para el modelado de tópicos
id2word = Dictionary(df_analisis['tokens'])

# Filtramos extremos
id2word.filter_extremes(no_below=2, no_above=.95)

# Creamos el corpus
corpus = [id2word.doc2bow(d) for d in df_analisis['tokens']]

# Encontramos el número óptimo de k
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):
   
    coherence_values_topic = []
    model_list_topic = []
    for num_topics in range(start, limit, step):
        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, passes=10, iterations=200)
        model_list_topic.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values_topic.append(coherencemodel.get_coherence())

    return model_list_topic, coherence_values_topic

# Después de llamar a la función `compute_coherence_values`
model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,
                                                                    corpus=corpus,
                                                                    texts=df_analisis['tokens'],
                                                                    start=2, limit=17, step=1)

#Se crea un csv con los valores coherencia del rango de tópicos establecido en el paso anterior
coherence_values_topic_df = pd.DataFrame(coherence_values_topic) 

coherence_values_topic_df = pd.DataFrame(coherence_values_topic) 
coherence_values_topic_df.to_csv('coherence_values.csv', index=False)

coherence_values_topic_df= pd.read_csv('coherence_values.csv', index_col=False)

coherence_values_topic_df

#Extraemos la primera columna del DataFrame y convertimos los valores en lista
coherence_values_topic_df=coherence_values_topic_df.iloc[:, 0].tolist()

#Ploteamos los resultados de los valores de coherencia de cada uno de los tópicos
import matplotlib.path as mpath
plt.figure(figsize=(10,7))
estrella = mpath.Path.unit_regular_star(6)
circulo = mpath.Path.unit_circle()
verts = np.concatenate([circulo.vertices, estrella.vertices[::-1, ...]])
codes = np.concatenate([circulo.codes, estrella.codes])
cut_star = mpath.Path(verts, codes)
plt.plot([2,3,4,5,6,7,8,9,10],coherence_values_topic_df[0:9], '--b', marker="o", markersize=10, fillstyle='none') #Seleccionamos un máximo de 10 tópicos para que nos muestre
plt.axvline(x=4, color='red', linestyle='--')
plt.xlabel('Número de tópicos')
plt.ylabel('Indice de coherencia')

#Se genera el modelo LDA con el número de k elegido, en este caso k=4
k=4
model_k9 = gensim.models.LdaMulticore(corpus=corpus, 
                                        id2word=id2word, 
                                        num_topics=k,
                                        passes=5)

#El código proporcionado guarda un modelo LDA (LdaMulticore) entrenado llamado "model_k7" en un archivo llamado "model9_7_topics.model". Luego, carga el modelo guardado desde el archivo "model9_7_topics.model" y lo asigna a la variable "model_k7_load".
#Para ejecutarlo correctamente, primero descomentamos la primera línea y ejecutamos; después, la comentamos y descomentamos la segunda y volvemos a ejecutar
model_k9.save("modelo_9_topics.model") 

model_k9_load=LdaMulticore.load("modelo_9_topics.model")

# Mostramos las palabras clave de nuestros 4 tópicos
print(model_k9_load.print_topics())
doc_lda = model_k9_load[corpus]
# Filtramos por palabras
words = [re.findall(r'"([^"]*)"',t[1]) for t in model_k9_load.print_topics()]
# Creamos tópicos
topics = [' '.join(t[0:10]) for t in words]
# Obtenemos los tópicos
for id, t in enumerate(topics): 
    print(f"------ Topic {id} ------")
    print(t, end="\n\n")

#Pintamos la distancia intertópica con pyLDAvis
pyLDAvis.enable_notebook()
gensimvis.prepare(model_k9_load, corpus, id2word)

#Se añade una columna de LDA features al modelo cargado
def document_to_lda_features(model_k4_load,document):
  topic_importance=np.array(model_k4_load.get_document_topics(document, minimum_probability=0))
  return topic_importance[:,1]

df_analisis['lda_features']=list(map(lambda doc: document_to_lda_features(model_k9_load,doc), corpus))

#Se tokeniza los datos
def topic_important(item_score):
    score=np.argmax(item_score, axis=0)
    return score

#Se aplica la tokenización
df_analisis['topic_dominant'] = df_analisis['lda_features'].apply(topic_important)
df_analisis

#Se genera un csv con los resultados
df_analisis.to_csv('topic_analisis_results1.csv', index=False)

#Se saca el número de tweets por tópico
df_analisis["topic_dominant"].value_counts()

#Dibujamos esa distribución
plt.figure(figsize=(10,7))
ax=df_analisis["topic_dominant"].value_counts().sort_index().plot(kind='bar')
plt.ylabel('Frecuencia Absoluta')
plt.title('Distribución de tópicos en el corpus textual')
plt.show()

##Bigramas por tópico
#Tópico 1
topic_1 = df_analisis[df_analisis['topic_dominant']==0] 
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes en el tópico 1')

#Tópico 2
topic_2 = df_analisis[df_analisis['topic_dominant']==1] 
dft=topic_2['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_analisis[df_analisis['topic_dominant']==2] 
dft=topic_3['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_analisis[df_analisis['topic_dominant']==3] 
dft=topic_4['lemmas']
tfIdfVectorizer_bi=TfidfVectorizer(use_idf=True, ngram_range=(2,2))
tfIdf_bi = tfIdfVectorizer_bi.fit_transform(dft)
names_bi=tfIdfVectorizer_bi.get_feature_names_out()
freqs_bi = tfIdf_bi.sum(axis=0).A1
result_bi= dict(zip(names_bi, freqs_bi))
from operator import itemgetter
i = 0
results_sorted=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)
#Se pinta
results_sorted_bi=sorted(result_bi.items(), key = itemgetter(1), reverse = True)
df_results_bi=pd.DataFrame.from_dict(results_sorted_bi).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_bi[0],df_results_bi[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Bigramas más relevantes del tópico 4')

##Trigramas por tópico
#Tópico 1
topic_1 = df_analisis[df_analisis['topic_dominant']==0] 
dft=topic_1['lemmas']
dft= [x for x in dft if str(x) != 'nan']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 1')

#Tópico 2
topic_2 = df_analisis[df_analisis['topic_dominant']==1] 
dft=topic_2['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 2')

#Tópico 3
topic_3 = df_analisis[df_analisis['topic_dominant']==2] 
dft=topic_3['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 3')

#Tópico 4
topic_4 = df_analisis[df_analisis['topic_dominant']==3] 
dft=topic_4['lemmas']
tfIdfVectorizer_tri=TfidfVectorizer(use_idf=True, ngram_range=(3,3))
tfIdf_tri = tfIdfVectorizer_tri.fit_transform(dft)
names_tri= tfIdfVectorizer_tri.get_feature_names_out()
freqs_tri = tfIdf_tri.sum(axis=0).A1
result_tri = dict(zip(names_tri, freqs_tri))
from operator import itemgetter
i = 0
results_sorted=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
for key, value in results_sorted:
    i += 1
    if i == 11:
      break
    print(key, value)

#Se pinta
results_sorted_tri=sorted(result_tri.items(), key = itemgetter(1), reverse = True)
df_results_tri=pd.DataFrame.from_dict(results_sorted_tri).head(30)
plt.rcParams.update({'font.size': 20})
plt.figure(figsize=(15,7))
plt.bar(df_results_tri[0],df_results_tri[1])
plt.xticks(rotation=90)
plt.ylabel('TF-IDF Score')
plt.title('Trigramas más relevantes del tópico 4')

##Análisis de Sentimientos
